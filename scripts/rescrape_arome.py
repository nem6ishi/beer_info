#!/usr/bin/env python3
"""
Aromeå°‚ç”¨å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
- Aromeã®å…¨ä»¶ã‚’å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—
- ä»–ã‚·ãƒ§ãƒƒãƒ—ã®æœ€å¤first_seenã‚ˆã‚Šå¤ã„ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨­å®š
- Aromeå†…ã§ã®æ–°ç€é †ã¯ä¿æŒ
"""
import asyncio
import os
import sys
from datetime import datetime, timedelta, timezone
from supabase import create_client, Client
from dotenv import load_dotenv

# Load environment variables
env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')
load_dotenv(env_path)

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.scrapers import arome

# Get Supabase credentials
SUPABASE_URL = os.getenv('SUPABASE_URL') or os.getenv('NEXT_PUBLIC_SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_SERVICE_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("Error: Supabase credentials must be set")
    sys.exit(1)


async def rescrape_arome():
    print("=" * 60)
    print("ğŸº Arome å†ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ— (first_seen ã‚’ä»–ã‚·ãƒ§ãƒƒãƒ—ã‚ˆã‚Šå¤ãè¨­å®š)")
    print("=" * 60)
    
    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
    
    # 1. ä»–ã‚·ãƒ§ãƒƒãƒ—ã®æœ€å¤ first_seen ã‚’å–å¾—
    print("\nğŸ“… ä»–ã‚·ãƒ§ãƒƒãƒ—ã®æœ€å¤ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å–å¾—ä¸­...")
    response = supabase.table('scraped_beers') \
        .select('first_seen') \
        .neq('shop', 'Arome') \
        .order('first_seen', desc=False) \
        .limit(1) \
        .execute()
    
    if response.data:
        oldest_other = datetime.fromisoformat(response.data[0]['first_seen'].replace('Z', '+00:00'))
        print(f"  ä»–ã‚·ãƒ§ãƒƒãƒ—ã®æœ€å¤: {oldest_other}")
    else:
        oldest_other = datetime.now(timezone.utc)
        print("  ä»–ã‚·ãƒ§ãƒƒãƒ—ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚ç¾åœ¨æ™‚åˆ»ã‚’åŸºæº–ã«ã—ã¾ã™ã€‚")
    
    # åŸºæº–æ™‚åˆ»: ä»–ã‚·ãƒ§ãƒƒãƒ—ã®æœ€å¤ã‚ˆã‚Š1æ—¥å‰
    base_time = oldest_other - timedelta(days=1)
    print(f"  Aromeã®åŸºæº–æ™‚åˆ»: {base_time}")
    
    # 2. æ—¢å­˜ã®Aromeãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆuntappd_urlã‚’ä¿æŒã™ã‚‹ãŸã‚ï¼‰
    print("\nğŸ“‚ æ—¢å­˜ã®Aromeãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
    existing_arome = []
    chunk_size = 1000
    start = 0
    
    while True:
        res = supabase.table('scraped_beers') \
            .select('url, untappd_url') \
            .eq('shop', 'Arome') \
            .range(start, start + chunk_size - 1) \
            .execute()
        
        if not res.data:
            break
        existing_arome.extend(res.data)
        if len(res.data) < chunk_size:
            break
        start += chunk_size
    
    existing_data = {item['url']: item for item in existing_arome}
    print(f"  æ—¢å­˜Arome: {len(existing_data)} ä»¶")
    
    # 3. Aromeã‚’ãƒ•ãƒ«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—
    print("\nğŸ” Arome ã‚’ãƒ•ãƒ«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ä¸­...")
    scraped_items = await arome.scrape_arome(limit=None, existing_urls=None, full_scrape=True)
    print(f"  ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—ä»¶æ•°: {len(scraped_items)} ä»¶")
    
    if not scraped_items:
        print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ—çµæœãŒç©ºã§ã™")
        return
    
    # 4. ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’å‰²ã‚Šå½“ã¦ï¼ˆå¤ã„é †ã«å‡¦ç†ï¼‰
    # scraped_items ã¯ æ–°ç€â†’å¤ã„ ã®é †ãªã®ã§ reverse
    items_to_process = list(reversed(scraped_items))
    
    current_time = datetime.now(timezone.utc)
    current_time_iso = current_time.isoformat()
    
    beers_to_upsert = []
    
    for i, item in enumerate(items_to_process):
        url = item.get('url')
        if not url:
            continue
        
        # ãƒã‚¤ã‚¯ãƒ­ç§’å˜ä½ã§å¢—åŠ ï¼ˆé †åºä¿æŒï¼‰
        item_time = base_time + timedelta(microseconds=i)
        item_time_iso = item_time.isoformat()
        
        beer_data = {
            'url': url,
            'name': item.get('name'),
            'price': item.get('price'),
            'image': item.get('image'),
            'stock_status': item.get('stock_status'),
            'shop': item.get('shop'),
            'first_seen': item_time_iso,
            'last_seen': current_time_iso,
        }
        
        # æ—¢å­˜ã®untappd_urlã‚’ä¿æŒ
        existing = existing_data.get(url)
        if existing and existing.get('untappd_url'):
            beer_data['untappd_url'] = existing.get('untappd_url')
        
        beers_to_upsert.append(beer_data)
    
    # 5. ãƒãƒƒãƒã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ
    if beers_to_upsert:
        batch_size = 500
        for i in range(0, len(beers_to_upsert), batch_size):
            batch = beers_to_upsert[i:i + batch_size]
            print(f"\nğŸ’¾ Upserting batch {i // batch_size + 1} ({len(batch)} beers)...")
            try:
                supabase.table('scraped_beers').upsert(batch, on_conflict='url').execute()
                print(f"  âœ… Upserted {len(batch)} beers")
            except Exception as e:
                print(f"  âŒ Error: {e}")
    
    print(f"\n{'='*60}")
    print(f"âœ… å®Œäº†: {len(beers_to_upsert)} ä»¶ã®Aromeãƒ“ãƒ¼ãƒ«ã‚’æ›´æ–°")
    print(f"   first_seen ç¯„å›²: {base_time} ã€œ {base_time + timedelta(microseconds=len(beers_to_upsert))}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(rescrape_arome())
